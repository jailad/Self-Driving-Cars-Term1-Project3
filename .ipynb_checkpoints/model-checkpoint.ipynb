{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "* A lot of the experimentation for this project was performed within 'Term1Project3BehaviorCloning' and 'SideExperiments''\n",
    "* However, as per Rubric requirements, there is a need for a finalized 'model' file, which contains the implementation.\n",
    "* Therefore, this file was created essentially as a condensed version of the above notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Common Imports\n",
    "\n",
    "import resource\n",
    "import numpy as np\n",
    "import csv\n",
    "import time # Used for calculating the run-time of code segments\n",
    "import matplotlib.pyplot as plt # Useful for generating training plots\n",
    "# Useful for inline plot generation\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Activation, Flatten, Dropout, Lambda, Input, Cropping2D, ELU\n",
    "from keras import backend as K\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "import pandas # For generating descriptive statistics\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import cv2 # Include Open CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Shared Constants\n",
    "\n",
    "const_separator_line = \"--------------------------------\"\n",
    "const_csv_filename = 'driving_log.csv'\n",
    "const_image_foldername = 'IMG/'\n",
    "const_udacity_data_folder_name = 'data/fromudacity/'\n",
    "const_my_data_fast_track1_clockwise_folder_name = 'data/fast_track1clockwise/'\n",
    "const_my_data_fast_track1_counterclockwise_folder_name = 'data/fast_track1counterclockwise/'\n",
    "const_my_data_track1_curves_cw_folder_name = 'data/mydata_track1_curves_cw/'\n",
    "const_my_data_track1_curves_ccw_folder_name = 'data/mydata_track1_curves_ccw/'\n",
    "\n",
    "\n",
    "const_validation_data_ratio = 0.5 # Split off 20% of samples for validation\n",
    "\n",
    "# Named constants for important parameter values\n",
    "\n",
    "const_dropout_probability = 0.2\n",
    "const_num_epochs = 3\n",
    "const_activation_function = 'relu'\n",
    "const_padding_strategy = 'valid'\n",
    "const_padding_strategy_same ='same'\n",
    "const_loss_function = 'mse'\n",
    "const_optimizer_function = 'adam'\n",
    "const_model_filename_prefix = 'model_'\n",
    "const_model_filename_postfix = '.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Useful to selectively turn on / off logging at different levels\n",
    "\n",
    "const_info_log_enabled = False\n",
    "def infoLog(logMessage, param_separator=None):\n",
    "    if const_info_log_enabled == True:\n",
    "        print(\"\")\n",
    "        if param_separator:\n",
    "            print(param_separator) \n",
    "        print(logMessage)\n",
    "\n",
    "const_debug_log_enabled = True\n",
    "def debugLog(logMessage, param_separator=None):\n",
    "    if const_debug_log_enabled == True:\n",
    "        print(\"\")\n",
    "        if param_separator:\n",
    "            print(param_separator) \n",
    "        print(logMessage)\n",
    "        \n",
    "const_warning_log_enabled = True\n",
    "def warningLog(logMessage, param_separator=None):\n",
    "    if const_warning_log_enabled == True:\n",
    "        print(\"\")\n",
    "        if param_separator:\n",
    "            print(param_separator) \n",
    "        print(logMessage)\n",
    "        \n",
    "const_error_log_enabled = True\n",
    "def errorLog(logMessage, param_separator=None):\n",
    "    if const_error_log_enabled == True:\n",
    "        print(\"\")\n",
    "        if param_separator:\n",
    "            print(param_separator) \n",
    "        print(logMessage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182820"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Convenience method to get and Print current memory usage\n",
    "\n",
    "def print_memory_usage():\n",
    "    infoLog(\"{} Kb\".format(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss),)\n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "    \n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1494366241.5593612\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convenience method to get Current Time\n",
    "\n",
    "def get_current_time():\n",
    "    current_time = time.time()\n",
    "    return current_time\n",
    "\n",
    "debugLog(get_current_time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2017-05-09T21:44:01.887048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Return Current time in human readable format\n",
    "\n",
    "def get_current_human_readable_time():\n",
    "    return datetime.datetime.now().isoformat()\n",
    "\n",
    "debugLog(get_current_human_readable_time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convenience method to get Print Time Difference\n",
    "\n",
    "def print_time_diff(start_time, end_time, activity_label=\"task\"):\n",
    "    time_difference = end_time - start_time\n",
    "    debugLog(\"Execution time for \" + activity_label + \" : \" + str(time_difference) + \" seconds\")\n",
    "    return time_difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convenience functions to generate different types of Keras model architectures\n",
    "\n",
    "# Defining convenience parameters for different network architectures\n",
    "# C = Convolution\n",
    "# A = Activation\n",
    "# P = Pooling\n",
    "# FL = Flatten\n",
    "# FC = Fully Connected\n",
    "# D = Dropout\n",
    "\n",
    "# Base Model with Preprocessing\n",
    "def get_base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Cropping2D(cropping=((70,20), (0,0)), input_shape=(160,320,3)))\n",
    "    model.add(Lambda(lambda x: x/255.0 - 0.5))\n",
    "    #     model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape=(160,320,3)))\n",
    "    return model\n",
    "\n",
    "# From - https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/\n",
    "# From - http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf\n",
    "# Changes relative to above architecture, if applicable, have been called out at the individual layers\n",
    "\n",
    "def get_nvidia_model():\n",
    "    model_desc_name = 'nvidia_cnn_model'\n",
    "    # Normalization/ Preprocessing\n",
    "    model = get_base_model()\n",
    "    \n",
    "    #CA24_5_2_VALID\n",
    "    model.add(Convolution2D(24, 5, 5, subsample=(2,2), border_mode=const_padding_strategy))\n",
    "    model.add(Activation(const_activation_function))\n",
    "    \n",
    "    #CA36_5_2_VALID\n",
    "    model.add(Convolution2D(36, 5, 5, subsample=(2,2), border_mode=const_padding_strategy))\n",
    "    model.add(Activation(const_activation_function))\n",
    "    \n",
    "    #CA48_5_2_VALID\n",
    "    model.add(Convolution2D(48, 5, 5, subsample=(2,2), border_mode=const_padding_strategy))\n",
    "    model.add(Activation(const_activation_function))\n",
    "    \n",
    "    #CA64_3_1_VALID\n",
    "    model.add(Convolution2D(64, 3, 3, subsample=(1,1), border_mode=const_padding_strategy))\n",
    "    model.add(Activation(const_activation_function))\n",
    "    \n",
    "    #CA64_3_1_VALID\n",
    "    model.add(Convolution2D(64, 3, 3, subsample=(1,1), border_mode=const_padding_strategy))\n",
    "    model.add(Activation(const_activation_function))\n",
    "    \n",
    "    #FL\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #FCA1164\n",
    "    model.add(Dense(1164))\n",
    "    model.add(Activation(const_activation_function))\n",
    "    \n",
    "    #FCA100\n",
    "    model.add(Dense(100))\n",
    "    model.add(Activation(const_activation_function))\n",
    "    \n",
    "    #FCA50\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation(const_activation_function))\n",
    "    \n",
    "    #D-0.7\n",
    "    # Dropout was not shown in the original architecture, but I added it to prevent over fitting\n",
    "    model.add(Dropout(const_dropout_probability))\n",
    "    \n",
    "    #FC1\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss=const_loss_function, optimizer=const_optimizer_function, metrics=['accuracy','precision','recall'])\n",
    "\n",
    "    return model, model_desc_name\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture for nvidia_cnn_model\n",
      "\n",
      "Layer 0\n",
      "\n",
      "<keras.layers.convolutional.Cropping2D object at 0x7ff05b144d30>\n",
      "\n",
      "Layer 1\n",
      "\n",
      "<keras.layers.core.Lambda object at 0x7ff05b144438>\n",
      "\n",
      "Layer 2\n",
      "\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7ff05b14f5f8>\n",
      "\n",
      "Layer 3\n",
      "\n",
      "<keras.layers.core.Activation object at 0x7ff05b721e10>\n",
      "\n",
      "Layer 4\n",
      "\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7ff05b161518>\n",
      "\n",
      "Layer 5\n",
      "\n",
      "<keras.layers.core.Activation object at 0x7ff05b68be10>\n",
      "\n",
      "Layer 6\n",
      "\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7ff05b68b588>\n",
      "\n",
      "Layer 7\n",
      "\n",
      "<keras.layers.core.Activation object at 0x7ff05b6afc88>\n",
      "\n",
      "Layer 8\n",
      "\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7ff05b6af400>\n",
      "\n",
      "Layer 9\n",
      "\n",
      "<keras.layers.core.Activation object at 0x7ff05a835b00>\n",
      "\n",
      "Layer 10\n",
      "\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7ff05a835278>\n",
      "\n",
      "Layer 11\n",
      "\n",
      "<keras.layers.core.Activation object at 0x7ff05a840ef0>\n",
      "\n",
      "Layer 12\n",
      "\n",
      "<keras.layers.core.Flatten object at 0x7ff05a850470>\n",
      "\n",
      "Layer 13\n",
      "\n",
      "<keras.layers.core.Dense object at 0x7ff05a3e1eb8>\n",
      "\n",
      "Layer 14\n",
      "\n",
      "<keras.layers.core.Activation object at 0x7ff05a3edc50>\n",
      "\n",
      "Layer 15\n",
      "\n",
      "<keras.layers.core.Dense object at 0x7ff05a3ed9e8>\n",
      "\n",
      "Layer 16\n",
      "\n",
      "<keras.layers.core.Activation object at 0x7ff05a40e240>\n",
      "\n",
      "Layer 17\n",
      "\n",
      "<keras.layers.core.Dense object at 0x7ff05a40e400>\n",
      "\n",
      "Layer 18\n",
      "\n",
      "<keras.layers.core.Activation object at 0x7ff05a5fb5f8>\n",
      "\n",
      "Layer 19\n",
      "\n",
      "<keras.layers.core.Dropout object at 0x7ff05a5fbcf8>\n",
      "\n",
      "Layer 20\n",
      "\n",
      "<keras.layers.core.Dense object at 0x7ff05a5f19b0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convenience method which takes a Keras Model, and prints it's architecture\n",
    "\n",
    "def print_model_architecture(keras_model, model_label):\n",
    "    debugLog(\"Model Architecture for \" + model_label)\n",
    "    layers = keras_model.layers\n",
    "    for index, layer in enumerate(layers):\n",
    "        debugLog(\"Layer \" + str(index))\n",
    "        debugLog(layer)\n",
    "        \n",
    "model, model_name = get_nvidia_model()\n",
    "print_model_architecture(model, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convenience method which takes a Keras History object and prints it\n",
    "\n",
    "def plot_keras_history_object(keras_history_object, model_label):\n",
    "    plt.plot(keras_history_object.history['loss'])\n",
    "    plt.plot(keras_history_object.history['val_loss'])\n",
    "    plt.title('Model : ' + model_label +  ' -> mean squared error loss')\n",
    "    plt.ylabel('mean squared error loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data/fast_track1clockwise/IMG/center_2017_05_05_13_51_02_269.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convenience method to convert a long absolute path to relative path\n",
    "\n",
    "sample_absolute_path = \"/Users/jai/Desktop/BitBucket/UDSDC/GitHubSubmissions/Self-Driving-Cars-Term1-Project3-BehavioralCloning/data/fast_track1clockwise/IMG/center_2017_05_05_13_51_02_269.jpg\"\n",
    "\n",
    "def get_relative_path(param_absolute_path):\n",
    "    string_split = param_absolute_path.split('/')\n",
    "    string_split_last4 = string_split[-4:]\n",
    "    relative_path = \"/\".join(string_split_last4)\n",
    "    return relative_path\n",
    "\n",
    "debugLog(get_relative_path(sample_absolute_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Generator approach for loading, processing and training data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started Data Reading :data/fromudacity/driving_log.csv\n",
      "\n",
      "Ended Data Reading :data/fromudacity/driving_log.csv\n",
      "\n",
      " # Training images : 6151\n",
      "\n",
      " # Validation images : 6152\n",
      "\n",
      " # Training labels : 6151\n",
      "\n",
      " # Validation labels : 6152\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convenience method to read a CSV file and return left, right, center image paths, and parameters like steering angle, throttle etc.\n",
    "\n",
    "def load_csv_file_all_data(filename, param_offset = 0.01):\n",
    "\n",
    "    debugLog(\"Started Data Reading :\" + filename)\n",
    "    paths = list()\n",
    "    steering_angles = list()\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        has_header = csv.Sniffer().has_header(csvfile.read(1024))\n",
    "        infoLog(\"Has Header is \" + str(has_header) + \" for :\" + filename)\n",
    "        csvfile.seek(0)  # rewind\n",
    "        header = csv.reader(csvfile)\n",
    "        if has_header:\n",
    "            next(header)  # skip header row\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for data_row in reader:\n",
    "                center = data_row[0].strip() # Stripping is important in case there are any unintended leading / trailing spaces in the data\n",
    "                left = data_row[1].strip()\n",
    "                right = data_row[2].strip()\n",
    "                steering = data_row[3]\n",
    "                throttle = data_row[4]\n",
    "                brake = data_row[5]\n",
    "                speed = data_row[6]\n",
    "                                \n",
    "                # Filtering Logic\n",
    "                steering_angle = float(steering)\n",
    "                \n",
    "                steering_left = steering_angle + param_offset\n",
    "                steering_right = steering_angle - param_offset\n",
    "\n",
    "                # Include all images which are under represented\n",
    "                if (steering_angle < -0.1 and steering_angle > float('-inf') ) or (steering_angle > 0.1 and steering_angle < float('inf')):\n",
    "                    paths.append(left)\n",
    "                    paths.append(center)\n",
    "                    paths.append(right)\n",
    "                    \n",
    "                    steering_angles.append(steering_left)\n",
    "                    steering_angles.append(steering)\n",
    "                    steering_angles.append(steering_right)\n",
    "\n",
    "                # Include 50% of images which are over represented\n",
    "                else:\n",
    "                    index = np.random.randint(0, 100)\n",
    "                    if index >= 0 and index < 33:\n",
    "                        paths.append(left)\n",
    "                        paths.append(center)\n",
    "                        paths.append(right)\n",
    "                    \n",
    "                        steering_angles.append(steering_left)\n",
    "                        steering_angles.append(steering)\n",
    "                        steering_angles.append(steering_right)\n",
    "                        \n",
    "    debugLog(\"Ended Data Reading :\" + filename)\n",
    "    return paths, steering_angles\n",
    "\n",
    "\n",
    "print_memory_usage()\n",
    "paths, steering_angles = load_csv_file_all_data('data/fromudacity/driving_log.csv')\n",
    "print_memory_usage()\n",
    "\n",
    "# Separate out data into training and validation\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(paths, steering_angles, test_size=const_validation_data_ratio, random_state=5)\n",
    "\n",
    "debugLog(\" # Training images : \" + str(len(X_train)))\n",
    "debugLog(\" # Validation images : \" + str(len(X_validation)))\n",
    "debugLog(\" # Training labels : \" + str(len(y_train)))\n",
    "debugLog(\" # Validation labels : \" + str(len(y_validation)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Writing a generator which yields an iterable 'batch_size' number of images from the above samples\n",
    "\n",
    "def load_images_generator(filepaths, labels, batch_size, prefix=None):\n",
    "    # The reason for infinity iteration -> http://stackoverflow.com/questions/37798410/why-does-this-python-generator-have-no-output-according-to-keras\n",
    "    while True:\n",
    "        offset = 0\n",
    "        for offset in range(0,len(filepaths),batch_size):\n",
    "            batch_images_paths = filepaths[offset:offset+batch_size]\n",
    "            batch_images = list()\n",
    "            batch_labels = labels[offset:offset+batch_size]\n",
    "            for image_path in batch_images_paths:\n",
    "                if prefix:\n",
    "                    image_path = prefix + image_path\n",
    "                else:\n",
    "                    image_path = get_relative_path(image_path)\n",
    "                image = cv2.imread(image_path)\n",
    "                batch_images.append(image)\n",
    "            batch_images = np.array(batch_images)\n",
    "            batch_labels = np.array(batch_labels)\n",
    "            yield sklearn.utils.shuffle(batch_images, batch_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convenience method to train a model given a specified CSV file path to the data set\n",
    "# The model can be a fresh ( randomly initialized ) model, or a model which has been saved and restored from the Disk\n",
    "# After the completion of training, the best model is saved to disk and also returned\n",
    "# the suffix alldata represents the fact that we are using Left, Right and Center Data to do the training and validation\n",
    "\n",
    "def train_model_for_csv_file_alldata(param_model, param_model_desc_name, csv_file_path, param_epochs = 2, param_steering_offset=0.02, param_batch_size = 64 ):\n",
    "        \n",
    "    debugLog(csv_file_path)\n",
    "    \n",
    "    path_prefix = 'data/fromudacity/'\n",
    "    # TODO: A bit awkward to separate Udacity Data From Data Generated by me, but works for now\n",
    "    if csv_file_path != const_udacity_data_csv_path:\n",
    "        path_prefix = None\n",
    "        debugLog(\"path_prefix is None because this is a non-udacity data file.\")\n",
    "    else:\n",
    "        debugLog(\"path_prefix is : \" + csv_file_path)\n",
    "    \n",
    "    # Load and Filter the Data - Only keep the Data we care about ( randomly drop oversampled data ) and return filtered arrays\n",
    "    paths, steering_angles = load_csv_file_all_data(csv_file_path, param_steering_offset)    \n",
    "    \n",
    "    \n",
    "    # Separate out data into training and validation\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(paths, steering_angles, test_size=const_validation_data_ratio, random_state=5)\n",
    "    debugLog(\" # Training images : \" + str(len(X_train)))\n",
    "    debugLog(\" # Validation images : \" + str(len(X_validation)))\n",
    "    debugLog(\" # Training labels : \" + str(len(y_train)))\n",
    "    debugLog(\" # Validation labels : \" + str(len(y_validation)))\n",
    "    \n",
    "    # Start training process\n",
    "\n",
    "    # Define Checkpoint and Early Stopping\n",
    "    model_filepath = const_model_filename_prefix + model_desc_name + str(get_current_human_readable_time()) + \"-{epoch:02d}-{val_acc:.2f}\" + const_model_filename_postfix\n",
    "    model_checkpoint = ModelCheckpoint(model_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=0)\n",
    "    model_callbacks_list = [model_checkpoint, early_stopping]\n",
    "\n",
    "    train_generator = load_images_generator(X_train, y_train, param_batch_size, path_prefix)\n",
    "    validation_generator = load_images_generator(X_validation, y_validation, param_batch_size, path_prefix)\n",
    "    start_time = get_current_time()\n",
    "    debugLog(get_current_human_readable_time())\n",
    "    model_history = param_model.fit_generator(train_generator, samples_per_epoch = len(X_train), validation_data = validation_generator, nb_val_samples = len(X_validation), nb_epoch = param_epochs, callbacks = model_callbacks_list)\n",
    "    debugLog(model_history.history.keys())\n",
    "    end_time = get_current_time()\n",
    "    debugLog(get_current_human_readable_time())\n",
    "    print_time_diff(start_time,end_time,\"Training \" + param_model_desc_name + \" for \" + str(param_epochs) + \" epochs\")    \n",
    "    # Generate plot of accuracy over epochs\n",
    "    plot_keras_history_object(model_history,param_model_desc_name)\n",
    "    \n",
    "    val_loss_str = \"val_loss_\" + str(model_history.history.get(\"val_loss\", \"unknown\"))\n",
    "    model_save_filename = const_model_filename_prefix + model_desc_name + str(get_current_human_readable_time()) + val_loss_str + const_model_filename_postfix\n",
    "    param_model.save(model_save_filename)\n",
    "    debugLog(\"Saved model with name : \" + model_save_filename)\n",
    "\n",
    "    # Return the Model\n",
    "    return param_model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the 'Nvidia Model'\n",
    "\n",
    "# Create a new model from scratch\n",
    "# nvidia_model, model_desc_name = get_nvidia_model()\n",
    "\n",
    "# or, Restore one from Disk\n",
    "model_path = 'model.h5'\n",
    "model_desc_name = 'nvidia_cnn_model'\n",
    "nvidia_model = load_model(model_path)\n",
    "\n",
    "# Print out the architecture\n",
    "debugLog(model_path)\n",
    "print_model_architecture(nvidia_model, model_desc_name)\n",
    "\n",
    "# Train Repeatedly for different data sets\n",
    "\n",
    "nvidia_model = train_model_for_csv_file_alldata(nvidia_model, model_desc_name, const_udacity_data_csv_path, 5 , 0.2, 100)\n",
    "# nvidia_model = train_model_for_csv_file_alldata(nvidia_model, model_desc_name, const_my_data_fast_track1_clockwise_csv_path, 1 , 0.08, 100)\n",
    "# nvidia_model = train_model_for_csv_file_alldata(nvidia_model, model_desc_name, const_my_data_fast_track1_counter_clockwise_csv_path, 1 , 0.08, 100)\n",
    "# nvidia_model = train_model_for_csv_file_alldata(nvidia_model, model_desc_name, const_my_data_track1_curves_cw_csv_path, 1 , 0.08, 100)\n",
    "# nvidia_model = train_model_for_csv_file_alldata(nvidia_model, model_desc_name, const_my_data_track1_curves_ccw_csv_path, 1 , 0.08, 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
